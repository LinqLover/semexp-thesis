% !TEX root = ../../semexp-thesis.tex

\section{Designing System Interfaces for Programmatic Access}
\label{sec:agent/interfaces}

\begin{table}
	\centering
	\footnotesize
	\thimport{02_interfaces/functions}
	\caption[The function interface that connects our exploratory programming agent to the Squeak"/Smalltalk system.]{
		The complete function interface that connects our exploratory programming agent to the Squeak"/Smalltalk system.
		Most browsing functions are designed to return extensive information, reducing the need for subsequential function calls.
	}
	\label{tab:agent/functions/interface}
\end{table}

To define a set of functions through which the agent can access the system, we imitate the actions that programmers can take in traditional exploratory programming systems.
For example, we allow the agent to inspect the state of an object similarly to an inspector tool by requesting variable values; browse the source code of the system similarly to a code browser by requesting its packages, classes, and methods; or send messages to objects by evaluating scripts in the context of the explored object.
\Cref{tab:agent/functions/interface} lists the complete function interface of our prototypical agent for Squeak"/Smalltalk.
Below, we discuss two challenges in designing these functions: the granularity of function calls and the language proficiency of the agent when evaluating code.

\paragraph{Granularity of function calls}
An important trade-off regards the amount of information that is requested through a single function call:
for example, when browsing a class, the agent could either first request a list of protocols in a class and then request the message names within relevant protocols, or request the entire list of message names for all protocols at once.
In our prototype, we generally choose a medium-to-coarse-grained function design because for many cloud-based LLMs such as OpenAI's GPT models, the cost of processed tokens is quadratic in the number of sequentially requested function calls (as every newly requested function call requires a new API request and processing of the prior conversation).

\paragraph{Evaluating code}
Another challenge is the limited proficiency of unspecialized LLMs concerning particular programming languages:
for example, when writing Smalltalk code, GPT-4 frequently produces syntax errors and shows insufficient knowledge of standard libraries such as the \code{Collections} package.
To support the model in correcting its own errors, we extend a small number of built-in error messages of a system with practical suggestions based on typical faults of the model~\cite{traver2010compiler}:
for example, because GPT-4 often forgets necessary brackets when chaining message sends, we extend all \code{MessageNotUnderstood} errors from the system with an explaining comment and provide an example of correcting an incorrect chain.

We also reduce the dependency of the agent on language proficiency by exposing most interfaces through dedicated functions rather than instructing the model to access them through evaluating reflection code.
For example, while it would be elegant to have the agent retrieve the protocols of a class by calling \code{eval("(Smalltalk at: aClassName) organization categories")}\footnote{A similar technique is used in ChatGPT Plus for browsing long documents.}, we offer a dedicated \code{browse\-Class()} function for this purpose instead.

To avoid dangerous side effects of AI-generated code, it is also possible to evaluate all requested scripts in a lightweight sandbox.
However, this imposes the additional complexity of managing different realities on the agent~\cite{rehwaldt2012exploring}, and the isolation layer can impact the overall performance of the agent.
In our prototype, we provided an option to disable the sandbox and left it disabled most of the time, as we only observed a small number of unintended side effects.
