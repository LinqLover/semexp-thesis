% !TEX root = ../../semexp-thesis.tex

\section{Potential of Semantic Technologies}
\label{sec:discussion/feasibility}

To deliver relevant suggestions and conversations for semantic tools, the semantic exploration kernel hinges on the capabilities of the underlying semantic technologies.
Here, we provide insights on using language models and other suggestion strategies for powering the suggestion engine and the exploratory programming agent of our prototype.

Over the past six months, we have successfully used our prototype of semantic suggestions and semantic conversations for several tasks of a similar size to that of the examples in \cref{cha:application} and \cref{fig:implementation/conversations/inspector}. % TODO: once we have completed screenshots for integration chapter, can we make this a range here or are the examples too theoretical?
At the same time, semantic interfaces have not yet become part of our daily used toolset, which we mainly trace back to the high monetary cost, limited accuracy, and noticeable delays of our current prototype as discussed in \cref{sec:discussion/performance} below.
Still, we believe that these observations are no fundamental limitations to our approach but can be improved through the onward evolution of faster and more powerful language models and systematic prompt engineering or fine-tuning.

\begin{genericbox}{Remark}
	This thesis presents our exploratory research on the integration of semantic technologies into programming systems.
	To test the general feasibility of our approach, we implemented a proof-of-concept prototype for central ideas.
	However, we did not make any systematic efforts such as tuning hyperparameters or prompts to improve the quality or performance of our semantic applications.
	Thus, any limitations noted in this discussion only refer to our prototype and do not necessarily indicate fundamental issues with the overall approach.
\end{genericbox}

\subsection{Semantic Retrieval}
\label{sec:discussion/feasibility/retrieval}

In our prototype of the suggestion engine, we implemented semantic retrieval for method and class artifacts.
For semantic suggestions and ranking, our implementation employs document embeddings as well as term-based methods (TF-IDF and correlation search).

In our experiments, we generally experienced suggestions from embedding-based search as helpful.
For example, we could reuse suggested similar implementations in many situations, discovered some unexpected duplications during code browsing, and were able to understand documentation commentary that was scattered about multiple methods in context.
However, the \code{text-embedding-3-large} model by OpenAI that we used offers limited capabilities for encoding abstract concepts of source code but overemphasizes specific vocabulary and document size.
This impedes semantic similarity search and semantic ranking, in particular reducing the relevance and irredundancy of ranked suggestions.
We believe that the fitness of embeddings could be improved by fine-tuning an embedding model for conceptual similarity, using ad-hoc fine-tuned text embeddings~\cite{su2023one}, or preprocessing documents with the help of another LLM prior to embedding them.

Being too sensitive to the document size, semantic search using embeddings also cannot be used to find related methods to an early unfinished draft of the programmer~(see \cref{sec:suggestions/search/similar}).
For this purpose, our TF-IDF search strategy yielded better results.
However, as TF-IDF only compares isolated literals, it does not support synonymous or homonymous identifiers, causing false negatives (missing suggestions) for similar implementations that use different names and false positives for implementations that overload names.
The same limitations apply to our (also term-based) correlation search.
We believe that by representing individual literals using concept-aware embeddings that include their semantics or reference graph~\cite{mattis2018semantic} instead of sparse TF-IDF embeddings, false negatives from synonyms could be reduced.

Semantic ranking using probabilistic sampling from clusters showed mixed results in our experiments.
In general, this strategy balanced relevance, irredundancy, diversity, and representativeness better than other approaches such as top-k selection~(see \cref{sec:suggestions/ranking}).
However, we noted an insufficient accuracy of relevance scores in many situations:
first, relevance scores from semantic similarity search underrepresent conceptual relevance in the ranking as described above.
Second, relevance scores from TF-IDF search are too imprecise for methods with few literals---for example, a method with a single selector that is also contained in the query will receive a perfect score of \qty{100}{\percent}.
To create a global ranking of artifacts from different search strategies, their relevance scores need to be normalized to a common scale.

\subsection{Exploratory Programming Agent}
\label{sec:discussion/feasibility/agent}

Due to their statistical foundations, LLMs and thus agents based on them are inherently subject to the risk of erring or failing.
To err means to provide wrong answers, either caused by limited reasoning abilities or by the tendency of LLMs to hallucinate~(see \cref{sec:background/semtec/generation}): for instance, our agent was unable in some cases to write executable scripts based on given source code or error messages, and it would occasionally send fictitious messages to objects.
To fail means to terminate without finding a solution: sometimes, our agent would stop after conducting a couple of unsuccessful experiments, stating that further knowledge about a system would be required.
In one example, the agent explicitly rejected the task of extracting an API key from the object graph of a version control system because it would be ``not allowed to provide confidential information''.

\begin{genericbox}{Examples}
	To quantify the abilities of our current agent, a larger evaluation based on representative data from actual exploratory programming sessions would be required, which is outside the scope of this work.
	However, we can illustrate its current state through a few anecdotical examples.

	For the questions about the \code{Text} object from \cref{sec:application/conversation}, the agent delivered correct and useful answers in approximately \qty{80}{\percent} of all requests.
	In the remaining cases, it would fail to locate the required interface for retrieving all attributes of the text but only return the attributes for the first character; reinvent the wheel and spend many times tokens and seconds more manually accessing the underlying data; or suggest non-functional or idiosyncratic code snippets for the third question.

	In the semantic debugger~(see \cref{fig:application/system/debugger}), the agent would sometimes be lazy and only answer the first part of our question until we insisted.

	We were able to research and apply different methods of Squeak's graphics framework Morphic through the agent to create, alter, and decorate a graphical widget.
	When we requested to add an animation, the agent would in some cases generate the correct code but store it under the wrong hook; in one attempt, it added an erroneous method to the widget that caused the Morphic environment to crash.

	On one occasion, we used the agent successfully to extract the daily activity on the squeak-dev mailing list\footnote{Using Squeak Inbox Talk, see \cref{fig:implementation/conversations/inspector}.} and export it to a CSV file.
	When we asked it to render the results as a scatter plot manually (Squeak does not provide a built-in diagram framework at the time of writing), it made a conversion mistake that corrupted the x-coordinates of all points in the plot.
	After we asked it to fix the mistake, it entangled itself into a never-ending chain of thought, questioning basics of the Smalltalk syntax, researching unrelated protocols, and modifying and complicating other parts of its scripts, before we stopped it after several minutes and \$10 spent (which significantly exceeds the usual costs of typical questions).
\end{genericbox}

In general, the agent operated most competently for smaller tasks related to popular domains.
However, in most situations, we could send follow-up messages to the conversation to retrieve the correct answer by refining the task, telling the agent what it did wrong, and providing it guidance through the solution with a varying degree of detail.

Several weaknesses of our agent are due to two limited capabilities of the \gptfouro model we used:
first, this model showed a limited proficiency regarding the Smalltalk programming language and standard libraries, and it often mixed up protocols from different Smalltalk dialects such as Squeak and Pharo.
Second and more importantly, despite our prompt, the model only applied the exploratory paradigm to a limited degree and was not eager enough to consider and try out a larger number of different classes and messages.
We believe that by training a model on an extended set of Squeak"/Smalltalk source code and fine-tuning it for exploratory practices, both these abilities could be further improved.%
\footnote{
	For comparison, ChatGPT's code interpreter shows a higher proficiency in writing Python code for computational notebooks, using proprietary libraries to summarize documents, and correcting its own coding errors.
	We argue that these are similar and trainable traits to the typical system interactions in exploratory programming.
}
Alternatively, even a more systematic approach to prompt engineering with extended chain-of-thought or few-shot prompts might improve these capabilities.

The limited language proficiency of the model was also a critical limitation when generating semantic completions:
despite being presented extensive examples of the syntax and relevant interfaces, the agent produced invalid completions in \qty{50}{\percent}--\qty{75}{\percent} of all cases.
While we could filter out these errors by dynamically testing completions, this reduces the quantity and the effective performance of suggestions substantially.
Completions generally exhibited an acceptable diversity over the different provided examples even with a naive prompt design.
Still, by chaining multiple prompts or increasing the temperature of the model, it would be possible to generate more diverse completions while sacrificing the performance or correctness of results.
For stage-2 generations~(see \cref{sec:suggestions/generation}), the agent generally usually provided satifactory results for recontextualizing completions to a similar user input even when using the less powerful model \gptfouromini; only if the distance between stage-1 generations and user inputs grew too large (because we did not re-run stage-1 generations frequently enough), the model would tend to hallucinate fictitious names again.
