% !TEX root = ../../semexp-thesis.tex

\section{Performance of Semantic Tools}
\label{sec:discussion/performance}

In our setting with cloud-based language models and local vector stores, resource consumption mainly manifests as response times in semantic retrieval and the exploratory programming agent, memory consumption for semantic corpora (vector stores), and monetary costs for using language model services.
While memory consumption did not exceed \qty{100}{MB} in our experiments, response times and monetary costs of our prototype indicate that semantic completions and conversations demand further optimizations~(\cref{tab:discussion/performance/overview}).

\begin{table}
	\centering
	\footnotesize
	\thimport{02_performance/overview}
	\caption[Main response times and memory consumptions for the tools in our prototype of the semantic workspace.]{
		Main response times and memory consumptions for the tools in our prototype of the semantic workspace.
		Scattering of values is caused by the varying complexity of tasks, different suggestion strategies employed, and the uncertain frequency with which programmers may use tools.
	}
	\label{tab:discussion/performance/overview}
\end{table}

\subsection*{Response Times}
\label{sec:discussion/performance/latencies}

\paragraph{Semantic retrieval}
\label{par:discussion/performance/latencies/retrieval}

For semantic retrieval, response times occur in two situations: searching similar artifacts and embedding new artifacts in a semantic corpus.
While our implementation of similarity search in the \semtex framework currently employs a linear in-memory search and does not use any indexes, it already performs fast enough for most use cases.
We conducted our experiments in a Squeak 6.1Alpha image without additional packages installed\footnote{Testing environment: Squeak6.1alpha \#23073 64-bit, SistaV1; OpenSmalltalk VM 20231218\-1441 (2.5G bytecodes/sec, 170M sends/sec); Ubuntu 22.04 (virtualized in VirtualBox); AMD Ryzen 7 2700X @ \qty{3.7}{GHz}.}, which contains about \num{65000} methods and \num{3000} classes.
Here, searching for the $k = 100$ most similar methods to an existing method took not longer than 150 milliseconds; searching for similar classes took about 10 milliseconds.

When performing a semantic search with an arbitrary query such as the contents of a workspace, the query needs to be embedded first, adding a latency between 400 milliseconds for short scripts (\qty{1}{SLOC}\footnote{SLOC: Source lines of code (excluding empty lines).}) and 750 milliseconds for long scripts (\qty{200}{SLOC}) in our experiments.
Analogously, embedding new or changed methods and classes in a corpus involves similar latencies, but these are performed in the background and are usually invisible to programmers.
After changing larger amounts of code, such as when installing a new package, we can embed many documents in a single API request and parallelize requests to balance elastic scaling and possible rate limits.
For example, in our (not perfectly balanced) prototype implementation, embedding the full \code{Collections} package of Squeak (about \num{3100} classes and \qty{1900}{SLOC}) takes 7 seconds in the background; embedding all methods in the standard image (when installing our prototype initially) takes about three minutes.

Our implementation of TF-IDF search for similar methods takes about 15 milliseconds per literal of the query method (correlation strength: $R^2 = 0.99$) in the standard image.
Since \qty{75}{\percent} of methods in the standard image contain not more than 6 literals, most queries will take fewer than 100 milliseconds; \qty{99}{\percent} of methods contain not more than 34 literals and can be queried for similar methods within less than 900 milliseconds.
However, a few methods may contain more than 500 literals (usually ``data methods'' that contain serialized objects such as images or sounds), causing query times of 10 seconds or more.

Correlation search and ranking typically consume 2--5 milliseconds only.
In total, a full pass of creating semantic suggestions for an open workspace typically requires between 500 and 700 milliseconds.
To reduce these latencies, we could use local, smaller embedding models and maintain indices for semantic and TF-IDF search.

\paragraph{Exploratory programming agent}
\label{par:discussion/performance/latencies/agent}

Generating semantic completions took between 10 and 15 seconds in our experiments, which is mainly caused by the large context we pass to the exploratory programming agent.
By optimizing the prompt, varying the amount of contextual information depending on the complexity of the problem, or pre-training an LLM for Smalltalk code completions, these latencies could be reduced.
While it would be possible to stream completions similarly to agent responses in semantic conversations, we generally cannot validate incomplete expressions or preview their results, which would hinder effective filtering of noise in the completion menu.
Stage-2 generations for recontextualization took between 1.1 and 1.3 seconds in our experiments due to their lower complexity.
This makes it realistic to recontextualize completions whenever programmers add or change a few words; however, for a higher interactivity, it would be necessary to employ even smaller, preferably local models for this stage or complement LLM-based recontextualization with non-semantic heuristics as proposed in \cref{sec:suggestions/generation}.

Response times for semantic conversations largely depend on the amount of context provided, such as previous messages in the conversation and required experiments:
for simple questions (e.g., the first two messages in \cref{fig:application/conversation/text}), response times usually vary between 2 and 4 seconds, while complex questions that involve sequential experiments (especially those caused by internal trial and error of the agent) can result in response times between 5 to 15 seconds or more in a few cases.
By streaming responses character-wise from the API, we reduce the delay before the start of the answer by factors between \qty{20}{\percent} and \qty{80}{\percent}.

\subsection*{Memory Consumption}
\label{sec:discussion/performance/memory}

Maintaining embeddings for larger corpora of documents can require substantial amounts of storage.
In the current implementation of \semtex, we store embeddings in the image memory and do not maintain additional indexes (but instead perform a linear search).
To balance memory consumption against precision, we shorten all document embeddings of OpenAI's \code{text-embedding-3-large} model (which supports truncating embedding vectors through Matryoshka Representation Learning~\cite{kusupati2024matryoshka}) from 3072 to 256 dimensions (each encoded as a 32-bit float).
This reduces the accuracy of semantic retrieval compared to the full embedding size by an average loss in relevance scores of about $\pm 0.05$.%
%\footnote{Christoph Thiede and Curt Kennedy: ``Re: It looks like 'text-embedding-3' embeddings are truncated"/scaled versions from higher dim version''. OpenAI Developer Forum, 2024-02-07. URL: \url{https://web.archive.org/web/20240413011050/https://community.openai.com/t/it-looks-like-text-embedding-3-embeddings-are-truncated-scaled-versions-from-higher-dim-version/602276\#post_14}}
\footnote{Christoph Thiede and Curt Kennedy: Discussion on the quality loss in similarity search when shortening embeddings. In: \emph{It Looks Like \code{text-embedding-3} Embeddings Are Truncated"/Scaled Versions from Higher Dim Version}. OpenAI Developer Forum, 2024-02-07. URL: \url{https://web.archive.org/web/20240413011050/https://community.openai.com/t/it-looks-like-text-embedding-3-embeddings-are-truncated-scaled-versions-from-higher-dim-version/602276\#post_14}.}

Using this configuration, a semantic corpus of all methods in the standard image consumes about \qty{90}{MB}, and a corpus of all classes consumes about \qty{4}{MB}.
It would be possible to further improve memory consumption while trading off accuracy or speed by further reducing embedding sizes or offloading embeddings to disk (and maintaining an index for them in the image memory).
Alternatively, training a smaller embedding model for our particular use case of conceptual similarity of Smalltalk code could allow to reduce memory consumption while maintaining a high accuracy.

\subsection*{Monetary Cost}
\label{sec:discussion/performance/money}

When using a cloud embedding model, monetary cost for semantic suggestions occurs when updating semantic corpora or embedding inputs such as current workspace contents for querying corpora.
Using OpenAI's \code{text-embedding-3-large} model, embedding all methods and classes in the standard image costs approximately \USD{0.85} (US dollars).
During our experiments within the last six months, regular code changes that we performed ourselves or installed as updates of the Squeak Trunk image accounted for less than \USD{0.05} per month.
Embedding an average method draft or workspace script for querying averaged to \qty{0.01}{\cent} (US cents), and even long scripts (\qty{200}{SLOC}) never exceeded costs of \qty{0.1}{\cent}.
Assuming a programmer would type continuously into a workspace and we would update suggestions every 5 seconds, this would generate costs of fewer than \USD{0.08} per hour.

Generating semantic completions is generally more expensive than creating suggestions because many generative language models require more resources than embedding models.
Using our unoptimized prompt design, generating $k = 10$ stage-1 completions costs about \qty{15}{\cent} and generating stage-2 completions for them costs about \qty{0.05}{\cent}.
If a programmer typed continuously into a workspace, we could update stage-1 completions every 30 seconds and recontextualize them every second for a price of $\USD{0.08}\text{ (for suggestions, see above)} + \USD{18} + \USD{1.8} \approx \USD{20}$ per hour.
These costs would be considered impractical in most settings and emphasize the need for optimizing completions through prompt tuning or smaller, fine-tuned models as noted above.

Monetary costs for semantic conversations vary depending on the complexity of questions and answers, ranging from \USD{0.10} to \USD{0.50} for simple questions but rising to several dollars when encountering a lot of trial and error for more challenging questions.
For a programmer who adopts semantic object interfaces in their everyday toolset, this would involve expenses of at least \USD{5} per hour even when estimating the number of questions a programmer expresses very conservatively as 10 per hour~\cite{kubelka2018road}; following Jevon's paradox, the actual cost could be many times higher as programmers might become negligent of their expenses.%
\footnote{
	We also experimented with OpenAI's \code{gpt-3.5-0125} model instead of \code{gpt-4o-2024-05-13}.
	While this would improve response times and reduce costs by 5 to 10 times, \gptthreefive models are noticeably less capable, requiring us to ask questions five times or more (due to the non-deterministic behavior of LLMs) before we would get a helpful answer, if at all.
	%(For comparison: using \gptfouro, the same---yet not representative---sample of questions led to satisfying answers in more than 80\% of requests.)
}

\ParSep

Considering the ongoing trend of more powerful, faster, and less energy-consuming language models being developed, we assume that the latencies and expenses for invoking the suggestion engine and the exploratory programming agent could become small enough to no longer pose a practical barrier for programmers and organizations with more affluent budgets.%
\footnote{We acknowledge the high variance in budgets worldwide. For example, a senior developer in Silicon Valley with a three-digit hourly wage might regularly afford semantic conversations or even semantic completions today, while an entry-level programmer in a developing economy who earns less than \USD{1} per hour could likely not even afford semantic suggestions.}
For illustration, during the last twelve months, OpenAI's respective flagship LLM (from \code{gpt-4-0613} to \code{gpt-4o-2024-08-06}) has improved by a factor of at least two in speed and by a factor of six in monetary cost.
At the same time, we see \emph{small language models} as a promising alternative to cloud-based models, as they could be trained to handle particular tasks such as generating Smalltalk code completions or engaging in exploratory programming competently and efficiently and could be run on commodity hardware with reduced response times~\cite{magister2023teaching}.
