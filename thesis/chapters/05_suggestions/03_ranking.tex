% !TEX root = ../../semexp-thesis.tex

\section{Semantic Ranking}
\label{sec:suggestions/ranking}

Many strategies produce a number of suggestions that is neither manageable for human programmers nor for exploratory programming agents using state-of-the-art LLMs.
This is because semantic search methods, unlike traditional search methods, usually do not provide a binary classifier for the relevance of a document but compute a continuous score (e.g., the cosine distance in embedding-based search).
Thus, we need to sort and filter suggestions before returning them to the requestor.
We address both requirements through a \emph{ranker} in the suggestion engine, which computes a ranking order for all suggestions and returns the first $k$ suggestions from this ranking to the requestor.

The requirements for ranking orders are various and depend on the context of the suggestions.
Some roles of suggestion artifacts, such as similar or correlated artifacts, have a relevance score to a reference artifact, which should influence their ranking (e.g., to select the most similar artifacts).
In other situations, such as when suggesting users of a class or method, suggestions should explicitly provide a broad overview of all available data and not respect any reference artifact.
Between both extremes, gradations exist: for example, the exploratory programming agent might request senders of a method in the context of a broader topic, such as users of the \code{WriteStream>>nextPut:} method in Squeak that are related to date-formatting tasks.
In this situation, both suggestions relevant to the problem and suggestions exemplifying different uses of the protocol are desired.

We identify four \emph{objectives} for ranking suggestions, which must be balanced differently depending on the context in which the suggestion engine is invoked.
While we describe each objective with a formula, we have not used these formulas for a rigorous quantitative evaluation of different ranking methods but only provide them for mathematical intuition.

\begin{description}
	\item[Relevance]
	A ranking must contain the most relevant suggestions from the corpus, such as those that are most similar to the reference artifact.

	Formally, we operationalize this objective by requiring that for every prefix $L$ of the ranking, a ranking method must maximize the sum of relevance scores $R_i$ from all contained artifacts:
	\begin{align}
		\text{Relevance}(L) = \sum_{i \in L} R_i
	\end{align}

	\item[Irredundancy]
	A ranking must not contain multiple highly similar suggestions that do not add value to the exploration, such as different almost identical versions of a method.

	We measure this objective by counting the number of suggestion pairs $(i, j)$ in the ranking that do not deceed a small threshold $\theta$ for their mutual similarity $S(i, j)$:
	\begin{align}
		\text{Irredundancy}(L) = | \{ i, j \in L \mid S(i, j) > \theta \} |
	\end{align}

	\item[Diversity]
	A ranking must contain diverse suggestions that display highly different types and topics from the corpus.

	We formalize the diversity of a ranking by summing up all mutual distances between two suggestions.
	Unlike irredundancy, which merely avoids duplicates, this objective explicitly promotes outliers from different regions of the embedding space:
	\begin{align}
		\text{Diversity}(L) = \sum_{i, j \in L} 1 - S(i, j)
	\end{align}

	\item[Representativeness]
	A ranking must represent the full corpus to the best possible extent by containing suggestions that cover the entire distribution of the corpus.

	While a comprehensive operationalization of this objective would provide little intuition, we can approach it by comparing statistical properties (such as the mean and standard deviation) of the ranking to that of the corpus and requiring the difference to be minimal.
	For a more elaborated measure, we could require a minimal Kantorovich distance~\cite{vaserstein1969markov}, which indicates the cost of transforming the distribution of the corpus into that of the ranking.
\end{description}

\noindent
To meet all requirements, we present four ranking methods and discuss how they balance all objectives differently~(\cref{fig:suggestions/ranking/method_objectives}): \emph{top-k selection}, \emph{probabilistic sampling}, \emph{clustering}, and \emph{probabilistic sampling from clusters}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{03_ranking/method_objectives.png}
	\caption[Comparing semantic ranking methods against different objectives.]{
		The discussed methods for semantic ranking achieve different trade-offs between our defined objectives.
		Top-k selection (\bold{\textcolor[HTML]{c00000}{red}}) delivers the most relevant ranking but selects redundant, homogeneous, and non-representative items.
		Probabilistic sampling (\bold{\textcolor[HTML]{e09600}{yellow}}) maximizes diversity, while clustering (\bold{\textcolor[HTML]{00b050}{green}}) can improve the irredundancy and representativeness of items.
		With probabilistic sampling from clusters (\bold{\textcolor[HTML]{004080}{blue}}), we find an acceptable balance of all objectives.

		Note that this sketch shows a broad comparison of methods only, and exact metrics would heavily depend on the distribution of original suggestions and on the choice of hyperparameters (such as temperature) for each method.
	}
	\label{fig:suggestions/ranking/method_objectives}
\end{figure}

\paragraph{Top-k selection}
We sort all suggestions based on their relevance score and select the $k$ most relevant artifacts.
%
This approach maximizes the relevance of the ranking.
However, it can also maximize the redundancy of results if the relevance score depends on the (transitive) similarity of artifacts.
At the same time, diversity and representativeness are typically minimized because the selection explicitly focuses on most similar artifacts.

\paragraph{Probabilistic sampling}
We convert the relevance scores of the corpus into a probability distribution by scaling each relevance score exponentially to a probability $p_i$:
\begin{align}
	p_i = \frac{\exp \frac{R_i}{T}}{\sum_j \exp \frac{R_j}{T}}
\end{align}
Based on this probability distribution, we randomly sample $k$ unique artifacts.
Through a temperature parameter $T$, the probability distribution can be sharpened or flattened, modeling the ranking closer to top-k selection or uniform random selection, respectively.
This approach corresponds to the \emph{softmax} function used in several machine learning applications~\cite{vaswani2017attention}.

In comparison with all other discussed methods, probabilistic sampling maximizes the diversity of the ranking if a medium to high temperature is used.
Similarly, the irredundancy or representativeness of results is slightly better than for top-k selection.
However, results are significantly less relevant to the query.

\paragraph{Clustering}
We cluster the corpus into $k$ groups by using a partition-based algorithm such as k-means.
From each cluster, we select the most representative artifact (i.e., the center of the cluster).

Cluster-based selection maximizes the irredundancy and improves the representativeness of the ranking.
However, it does not honor the relevance of artifacts.
Also, cluster centers present a lower diversity than probabilistic sampling as they neglect outliers.

\paragraph{Probabilistic sampling from clusters}
We group the corpus into $k$ clusters, assign a probability to each cluster based on the relevance score of its center, and sample random clusters.
For each occurrence of a cluster, we select the next most relevant artifact from it.
Similar to plain probabilistic sampling, this method can be controlled through a temperature parameter.

Cluster-based probabilistic sampling provides a trade-off between the other discussed methods with respect to all considered objectives:
in our experiments, it yielded a moderate relevance of suggestions that is between that of top-k selection and plain probabilistic sampling, a moderate diversity close to clustering, and a sufficient irredundancy and representativeness.

\begin{genericbox}{Failed attempts}
	We also experimented with two other ranking methods that did not yield usable results for us:

	\subparagraph{Explicitly amplifying diversity}
	We computed a diversity score for each artifact as its average distance to all other artifacts and added that to the artifact score.
	Despite experimenting with different weights and scalings, we were not able to create rankings that would combine relevance and diversity in a useful way.

	\subparagraph{Clustering from an RBF-transformed embedding space}
	We transformed the embedding space of the corpus using a radial basis function (RBF) centered around the reference artifact before clustering it.
	We could not confirm our hypothesis that this approach would balance relevance and representativeness.
	In our experiments, the complexity and sensitivity of parameters rendered this approach impractical.
\end{genericbox}

%\ParSep

In our prototype, we found probabilistic sampling from clusters to balance the different objectives in the most useful way and to be versatile for most applications.
For some use cases where no reference artifact has been specified, we use the plain clustering method instead.
